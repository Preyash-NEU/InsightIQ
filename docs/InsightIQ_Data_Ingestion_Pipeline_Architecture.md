# InsightIQ - Data Ingestion Pipeline Architecture
**Version:** 2.0  
**Last Updated:** December 25, 2024  
**Focus:** Robust Multi-Layer Data Processing

---

## ğŸ“‹ Table of Contents
1. [Pipeline Overview](#pipeline-overview)
2. [Current Challenges](#current-challenges)
3. [Proposed Architecture](#proposed-architecture)
4. [Layer-by-Layer Design](#layer-by-layer-design)
5. [Data Quality Framework](#data-quality-framework)
6. [Implementation Strategy](#implementation-strategy)

---

## ğŸ¯ Pipeline Overview

### **Goal**
Transform unpredictable, messy user data into clean, normalized, analysis-ready datasets that ensure accurate AI-powered insights.

### **Key Principles**
1. **Defensive Programming**: Assume all user data is problematic
2. **Fail Gracefully**: Never crash on bad data, always provide feedback
3. **Preserve Original**: Keep raw data, store cleaned versions separately
4. **Transparency**: Show users what was cleaned/changed
5. **Reversibility**: Allow users to see original vs cleaned data
6. **Performance**: Process efficiently even for large files (100MB+)

---

## ğŸš¨ Current Challenges

### **Data Quality Issues**

#### 1. **Missing Values**
```
Problems:
- Excel: Empty cells, "N/A", "#N/A", "null", "-", ""
- CSV: Empty strings, "NaN", "NULL", "None", "?"
- Databases: NULL values, empty strings, default placeholders
```

#### 2. **Data Type Inconsistencies**
```
Problems:
- Numbers stored as text: "1,234.56" (with comma)
- Dates in various formats: "2024-01-01", "01/01/2024", "Jan 1, 2024"
- Boolean values: "Yes/No", "Y/N", "True/False", "1/0", "T/F"
- Mixed types in same column: [123, "456", null, "N/A"]
```

#### 3. **Encoding Issues**
```
Problems:
- Non-UTF-8 characters: special symbols, emoji, foreign languages
- Hidden characters: \r\n, \t, non-breaking spaces
- BOM (Byte Order Mark) in CSV files
```

#### 4. **Column Name Problems**
```
Problems:
- Spaces: "Revenue Amount", "Order ID"
- Special chars: "Revenue ($)", "Date/Time", "Customer#"
- Case inconsistency: "customer_id" vs "Customer_ID"
- Duplicate names: "Total", "Total.1", "Total.2"
```

#### 5. **Value Formatting**
```
Problems:
- Currency: "$1,234.56", "1234.56 USD", "â‚¬1.234,56"
- Percentages: "45%", "0.45", "45.0"
- Phone numbers: "(555) 123-4567", "555-123-4567", "+1-555-123-4567"
- Emails: Mixed case, spaces, invalid formats
```

#### 6. **Structural Issues**
```
Problems:
- Multi-header rows (Excel)
- Merged cells (Excel)
- Empty rows/columns
- Inconsistent row lengths
- Footer rows with totals/summaries
```

#### 7. **Database-Specific**
```
Problems:
- Foreign key references to non-existent records
- Stale data (not updated regularly)
- Inconsistent JOIN results
- Large BLOB/TEXT fields
- Timezone issues in TIMESTAMP columns
```

---

## ğŸ—ï¸ Proposed Architecture

### **7-Layer Pipeline**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  USER INPUT                                                  â”‚
â”‚  CSV, Excel, JSON, Parquet, TSV, PostgreSQL, MySQL, SQLite  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 1: INGESTION & RAW STORAGE                           â”‚
â”‚  - Read file/database                                       â”‚
â”‚  - Detect encoding, format                                  â”‚
â”‚  - Store raw copy (immutable)                               â”‚
â”‚  - Generate data fingerprint (hash)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 2: STRUCTURAL VALIDATION                             â”‚
â”‚  - Check file integrity                                     â”‚
â”‚  - Validate schema                                          â”‚
â”‚  - Detect multi-headers, merged cells                       â”‚
â”‚  - Remove empty rows/columns                                â”‚
â”‚  - Flag structural issues                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 3: COLUMN NORMALIZATION                              â”‚
â”‚  - Standardize column names                                 â”‚
â”‚  - Remove special characters                                â”‚
â”‚  - Handle duplicates                                        â”‚
â”‚  - Create column mapping (original â†’ normalized)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 4: DATA TYPE DETECTION & CASTING                     â”‚
â”‚  - Smart type inference                                     â”‚
â”‚  - Handle mixed types                                       â”‚
â”‚  - Parse dates intelligently                                â”‚
â”‚  - Convert currencies, percentages                          â”‚
â”‚  - Boolean normalization                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 5: DATA CLEANING & IMPUTATION                        â”‚
â”‚  - Handle missing values (multiple strategies)              â”‚
â”‚  - Remove duplicates                                        â”‚
â”‚  - Detect & handle outliers                                 â”‚
â”‚  - Trim whitespace                                          â”‚
â”‚  - Standardize text (case, encoding)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 6: DATA QUALITY ASSESSMENT                           â”‚
â”‚  - Calculate completeness (% non-null)                      â”‚
â”‚  - Check uniqueness (% unique values)                       â”‚
â”‚  - Detect patterns (regex, distributions)                   â”‚
â”‚  - Generate quality score per column                        â”‚
â”‚  - Flag low-quality columns                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 7: OPTIMIZED STORAGE & INDEXING                      â”‚
â”‚  - Convert to Parquet (columnar, compressed)                â”‚
â”‚  - Create indices for common queries                        â”‚
â”‚  - Store metadata (quality report, column info)             â”‚
â”‚  - Cache preview samples                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ANALYSIS-READY DATA                                        â”‚
â”‚  - Clean, normalized, typed                                 â”‚
â”‚  - Quality-scored                                           â”‚
â”‚  - Ready for AI queries                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š Layer-by-Layer Design

### **LAYER 1: Ingestion & Raw Storage**

**Purpose:** Safely intake data and preserve original

**Operations:**
```python
class IngestionLayer:
    def process(self, source):
        # 1. Detect source type
        source_type = self.detect_source_type(source)
        
        # 2. Read with encoding detection
        raw_data = self.read_with_encoding_detection(source)
        
        # 3. Generate fingerprint
        data_hash = self.generate_fingerprint(raw_data)
        
        # 4. Store raw copy (immutable)
        self.store_raw(raw_data, data_hash)
        
        # 5. Extract basic metadata
        metadata = {
            'source_type': source_type,
            'row_count': len(raw_data),
            'column_count': len(raw_data.columns),
            'file_size': source.size,
            'encoding': self.detected_encoding,
            'fingerprint': data_hash
        }
        
        return raw_data, metadata
```

**Key Features:**
- Auto-detect encoding (UTF-8, Latin-1, CP1252, etc.)
- Handle BOM (Byte Order Mark)
- Detect delimiter for CSV (comma, semicolon, tab, pipe)
- Read Excel with specific sheet selection
- Connect to databases with proper drivers
- Generate SHA-256 hash for deduplication

---

### **LAYER 2: Structural Validation**

**Purpose:** Ensure data has valid structure

**Operations:**
```python
class StructuralValidationLayer:
    def process(self, df):
        issues = []
        
        # 1. Check for multi-header rows
        if self.has_multi_headers(df):
            df, header_info = self.extract_multi_headers(df)
            issues.append("Multi-header rows detected and merged")
        
        # 2. Remove empty rows/columns
        empty_rows = df.isna().all(axis=1).sum()
        empty_cols = df.isna().all(axis=0).sum()
        df = df.dropna(how='all').dropna(axis=1, how='all')
        
        if empty_rows > 0:
            issues.append(f"Removed {empty_rows} empty rows")
        if empty_cols > 0:
            issues.append(f"Removed {empty_cols} empty columns")
        
        # 3. Check for footer rows (totals, summaries)
        footer_rows = self.detect_footer_rows(df)
        if footer_rows:
            df = df.iloc[:-len(footer_rows)]
            issues.append(f"Removed {len(footer_rows)} footer rows")
        
        # 4. Validate consistent row length
        if self.has_inconsistent_rows(df):
            df = self.fix_inconsistent_rows(df)
            issues.append("Fixed inconsistent row lengths")
        
        return df, issues
```

**Validations:**
- Detect multi-header rows (common in Excel)
- Remove completely empty rows/columns
- Identify footer rows (totals, summaries)
- Fix jagged arrays (inconsistent column counts)
- Detect merged cells and split them

---

### **LAYER 3: Column Normalization**

**Purpose:** Standardize column names for consistency

**Operations:**
```python
class ColumnNormalizationLayer:
    def process(self, df):
        column_mapping = {}
        
        for col in df.columns:
            # Original name
            original = col
            
            # Normalize
            normalized = (
                str(col)
                .strip()                          # Remove leading/trailing spaces
                .lower()                          # Lowercase
                .replace(' ', '_')                # Spaces to underscores
                .replace('/', '_')                # Slashes to underscores
                .replace('(', '')                 # Remove parentheses
                .replace(')', '')
                .replace('$', 'dollars')          # Currency symbols
                .replace('%', 'percent')
                .replace('#', 'number')
                .replace('-', '_')                # Hyphens to underscores
            )
            
            # Remove special characters
            normalized = re.sub(r'[^\w\s_]', '', normalized)
            
            # Remove multiple underscores
            normalized = re.sub(r'_+', '_', normalized)
            
            # Remove leading/trailing underscores
            normalized = normalized.strip('_')
            
            # Handle duplicates
            if normalized in column_mapping.values():
                i = 1
                while f"{normalized}_{i}" in column_mapping.values():
                    i += 1
                normalized = f"{normalized}_{i}"
            
            column_mapping[original] = normalized
        
        # Rename columns
        df = df.rename(columns=column_mapping)
        
        return df, column_mapping
```

**Transformations:**
- Convert to lowercase
- Replace spaces with underscores
- Remove special characters
- Handle duplicates (append _1, _2, etc.)
- Store original â†’ normalized mapping

**Examples:**
```
"Revenue ($)"       â†’ "revenue_dollars"
"Customer ID"       â†’ "customer_id"
"Order Date/Time"   â†’ "order_date_time"
"Total %"           â†’ "total_percent"
"Item#"             â†’ "item_number"
```

---

### **LAYER 4: Data Type Detection & Casting**

**Purpose:** Intelligently detect and convert data types

**Operations:**
```python
class DataTypeLayer:
    def process(self, df):
        type_info = {}
        
        for col in df.columns:
            # Get column data
            series = df[col]
            
            # Detect type
            detected_type = self.detect_column_type(series)
            
            # Cast to appropriate type
            df[col], conversion_info = self.cast_column(series, detected_type)
            
            type_info[col] = {
                'detected_type': detected_type,
                'conversion_success_rate': conversion_info['success_rate'],
                'failed_conversions': conversion_info['failed_count']
            }
        
        return df, type_info
    
    def detect_column_type(self, series):
        # Remove null values for type detection
        non_null = series.dropna()
        
        if len(non_null) == 0:
            return 'string'
        
        # Sample for performance
        sample = non_null.sample(min(1000, len(non_null)))
        
        # Try conversions in order of specificity
        if self.is_boolean(sample):
            return 'boolean'
        elif self.is_integer(sample):
            return 'integer'
        elif self.is_float(sample):
            return 'float'
        elif self.is_datetime(sample):
            return 'datetime'
        elif self.is_date(sample):
            return 'date'
        elif self.is_currency(sample):
            return 'currency'
        elif self.is_percentage(sample):
            return 'percentage'
        elif self.is_email(sample):
            return 'email'
        elif self.is_url(sample):
            return 'url'
        else:
            return 'string'
```

**Smart Type Conversions:**

**1. Boolean:**
```python
def parse_boolean(value):
    if pd.isna(value):
        return np.nan
    
    str_val = str(value).strip().lower()
    
    # True values
    if str_val in ['true', 't', 'yes', 'y', '1', 'on', 'enabled']:
        return True
    
    # False values
    if str_val in ['false', 'f', 'no', 'n', '0', 'off', 'disabled']:
        return False
    
    # Ambiguous
    return np.nan
```

**2. Numeric with Currency:**
```python
def parse_currency(value):
    if pd.isna(value):
        return np.nan
    
    # Remove currency symbols and commas
    cleaned = str(value).replace('$', '').replace('â‚¬', '').replace('Â£', '')
    cleaned = cleaned.replace(',', '').strip()
    
    try:
        return float(cleaned)
    except:
        return np.nan
```

**3. Dates (Multiple Formats):**
```python
def parse_date(value):
    if pd.isna(value):
        return pd.NaT
    
    # Try multiple date formats
    formats = [
        '%Y-%m-%d',           # 2024-01-15
        '%m/%d/%Y',           # 01/15/2024
        '%d/%m/%Y',           # 15/01/2024
        '%Y/%m/%d',           # 2024/01/15
        '%b %d, %Y',          # Jan 15, 2024
        '%B %d, %Y',          # January 15, 2024
        '%d-%b-%Y',           # 15-Jan-2024
        '%Y%m%d',             # 20240115
    ]
    
    for fmt in formats:
        try:
            return pd.to_datetime(value, format=fmt)
        except:
            continue
    
    # Fallback to pandas auto-detection
    try:
        return pd.to_datetime(value)
    except:
        return pd.NaT
```

**4. Percentage:**
```python
def parse_percentage(value):
    if pd.isna(value):
        return np.nan
    
    str_val = str(value).strip()
    
    # Remove % sign
    if '%' in str_val:
        cleaned = str_val.replace('%', '').strip()
        try:
            return float(cleaned) / 100.0  # Convert to decimal
        except:
            return np.nan
    
    # Already decimal (0.45)
    try:
        num = float(str_val)
        if 0 <= num <= 1:
            return num
        else:
            return num / 100.0  # Assume it's percentage (45 â†’ 0.45)
    except:
        return np.nan
```

---

### **LAYER 5: Data Cleaning & Imputation**

**Purpose:** Handle missing values and clean data

**Operations:**
```python
class DataCleaningLayer:
    def process(self, df, type_info):
        cleaning_report = {}
        
        for col in df.columns:
            col_type = type_info[col]['detected_type']
            
            # 1. Handle missing values
            original_nulls = df[col].isna().sum()
            df[col], imputation_method = self.handle_missing_values(
                df[col], col_type
            )
            final_nulls = df[col].isna().sum()
            
            # 2. Remove duplicates (for ID columns)
            if self.is_id_column(col):
                duplicates = df[col].duplicated().sum()
                if duplicates > 0:
                    df = df.drop_duplicates(subset=[col], keep='first')
            
            # 3. Handle outliers (for numeric columns)
            if col_type in ['integer', 'float', 'currency']:
                outliers_count, df[col] = self.handle_outliers(df[col])
            else:
                outliers_count = 0
            
            # 4. Trim whitespace (for string columns)
            if col_type == 'string':
                df[col] = df[col].str.strip()
            
            cleaning_report[col] = {
                'original_nulls': original_nulls,
                'imputed_nulls': original_nulls - final_nulls,
                'imputation_method': imputation_method,
                'outliers_handled': outliers_count
            }
        
        return df, cleaning_report
```

**Missing Value Strategies:**

**1. Numeric Columns:**
```python
def handle_numeric_missing(series):
    # Strategy based on distribution
    if series.nunique() < 10:
        # Few unique values â†’ mode
        return series.fillna(series.mode()[0]), 'mode'
    elif series.skew() > 1 or series.skew() < -1:
        # Skewed distribution â†’ median
        return series.fillna(series.median()), 'median'
    else:
        # Normal distribution â†’ mean
        return series.fillna(series.mean()), 'mean'
```

**2. Categorical Columns:**
```python
def handle_categorical_missing(series):
    # Use most frequent value
    if series.mode().empty:
        return series.fillna('Unknown'), 'default'
    else:
        return series.fillna(series.mode()[0]), 'mode'
```

**3. Date Columns:**
```python
def handle_date_missing(series):
    # Forward fill (assume same as previous)
    return series.fillna(method='ffill'), 'forward_fill'
```

**Outlier Detection:**
```python
def handle_outliers(series, method='iqr'):
    if method == 'iqr':
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        # Clip outliers
        outliers = ((series < lower_bound) | (series > upper_bound)).sum()
        series = series.clip(lower=lower_bound, upper=upper_bound)
        
        return outliers, series
```

---

### **LAYER 6: Data Quality Assessment**

**Purpose:** Calculate quality metrics and flag issues

**Operations:**
```python
class DataQualityLayer:
    def process(self, df, type_info, cleaning_report):
        quality_report = {
            'overall_score': 0,
            'columns': {}
        }
        
        for col in df.columns:
            metrics = self.calculate_column_quality(df[col], type_info[col])
            
            quality_report['columns'][col] = {
                'completeness': metrics['completeness'],
                'uniqueness': metrics['uniqueness'],
                'consistency': metrics['consistency'],
                'validity': metrics['validity'],
                'quality_score': metrics['score'],
                'issues': metrics['issues']
            }
        
        # Overall dataset quality
        column_scores = [c['quality_score'] for c in quality_report['columns'].values()]
        quality_report['overall_score'] = np.mean(column_scores)
        
        return quality_report
    
    def calculate_column_quality(self, series, type_info):
        metrics = {}
        issues = []
        
        # 1. Completeness (% non-null)
        completeness = (1 - series.isna().sum() / len(series)) * 100
        metrics['completeness'] = completeness
        
        if completeness < 70:
            issues.append(f"Low completeness: {completeness:.1f}%")
        
        # 2. Uniqueness (% unique values)
        uniqueness = (series.nunique() / len(series)) * 100
        metrics['uniqueness'] = uniqueness
        
        # 3. Consistency (pattern matching)
        if type_info['detected_type'] == 'email':
            valid_emails = series.str.match(r'^[\w\.-]+@[\w\.-]+\.\w+$').sum()
            consistency = (valid_emails / len(series.dropna())) * 100
        elif type_info['detected_type'] == 'url':
            valid_urls = series.str.match(r'^https?://').sum()
            consistency = (valid_urls / len(series.dropna())) * 100
        else:
            consistency = 100  # Assume consistent
        
        metrics['consistency'] = consistency
        
        if consistency < 80:
            issues.append(f"Low consistency: {consistency:.1f}%")
        
        # 4. Validity (type-specific checks)
        if type_info['detected_type'] in ['integer', 'float']:
            # Check for unrealistic values
            if series.min() < 0 and series.max() > 0:
                validity = 100  # Both positive and negative OK
            else:
                validity = 100
        else:
            validity = 100
        
        metrics['validity'] = validity
        
        # 5. Overall quality score
        metrics['score'] = (
            completeness * 0.4 +
            consistency * 0.3 +
            validity * 0.2 +
            (min(uniqueness, 100) * 0.1)  # Cap at 100
        )
        
        metrics['issues'] = issues
        
        return metrics
```

**Quality Metrics:**
- **Completeness**: % of non-null values (target: >90%)
- **Uniqueness**: % of unique values (varies by column type)
- **Consistency**: Pattern matching for structured data
- **Validity**: Type-specific validation
- **Overall Score**: Weighted average (0-100)

**Quality Flags:**
```python
quality_flags = {
    'excellent': score >= 90,
    'good': 80 <= score < 90,
    'fair': 70 <= score < 80,
    'poor': 60 <= score < 70,
    'critical': score < 60
}
```

---

### **LAYER 7: Optimized Storage & Indexing**

**Purpose:** Store cleaned data efficiently for fast queries

**Operations:**
```python
class StorageLayer:
    def process(self, df, metadata, quality_report):
        # 1. Convert to Parquet (columnar format, compressed)
        parquet_path = self.get_parquet_path(metadata['source_id'])
        df.to_parquet(
            parquet_path,
            engine='pyarrow',
            compression='snappy',
            index=False
        )
        
        # 2. Create preview sample (first 1000 rows)
        preview_path = self.get_preview_path(metadata['source_id'])
        df.head(1000).to_parquet(preview_path, index=False)
        
        # 3. Store metadata
        metadata_extended = {
            **metadata,
            'quality_report': quality_report,
            'storage_format': 'parquet',
            'compression': 'snappy',
            'parquet_path': parquet_path,
            'preview_path': preview_path
        }
        
        # 4. Update database with metadata
        self.update_data_source_metadata(metadata_extended)
        
        return metadata_extended
```

**Storage Benefits:**
- **Parquet format**: Columnar storage (10x smaller than CSV)
- **Snappy compression**: Fast compression (2-5x size reduction)
- **Type preservation**: Maintains data types (no parsing needed)
- **Fast queries**: Read only needed columns
- **Preview caching**: First 1000 rows for instant display

---

## ğŸ¯ Data Quality Framework

### **Quality Score Calculation**

```python
def calculate_quality_score(column_data):
    weights = {
        'completeness': 0.40,   # Most important
        'consistency': 0.30,    # Pattern matching
        'validity': 0.20,       # Type-specific checks
        'uniqueness': 0.10      # Varies by use case
    }
    
    score = (
        column_data['completeness'] * weights['completeness'] +
        column_data['consistency'] * weights['consistency'] +
        column_data['validity'] * weights['validity'] +
        min(column_data['uniqueness'], 100) * weights['uniqueness']
    )
    
    return score
```

### **Quality Thresholds**

| Score Range | Quality Level | Action |
|-------------|---------------|--------|
| 90-100 | Excellent âœ… | Ready for analysis |
| 80-89 | Good âš ï¸ | Usable with minor issues |
| 70-79 | Fair âš ï¸ | Review flagged columns |
| 60-69 | Poor âŒ | Significant cleaning needed |
| 0-59 | Critical âŒ | Not suitable for analysis |

---

## ğŸ› ï¸ Implementation Strategy

### **Phase 1: Core Pipeline (Week 1)**
```
Priority: High
- Implement Layers 1-3 (Ingestion, Validation, Normalization)
- Basic type detection (Layer 4)
- Store raw + cleaned data
```

### **Phase 2: Advanced Cleaning (Week 2)**
```
Priority: High
- Complete Layer 4 (Smart type conversions)
- Implement Layer 5 (Missing value handling, outliers)
- Add cleaning reports
```

### **Phase 3: Quality & Storage (Week 3)**
```
Priority: Medium
- Implement Layer 6 (Quality metrics)
- Optimize Layer 7 (Parquet storage)
- Add quality dashboard in UI
```

### **Phase 4: User Feedback (Week 4)**
```
Priority: Medium
- Show cleaning reports to users
- Allow manual overrides
- Add data quality visualizations
- Implement column-level actions
```

---

## ğŸ“Š Data Quality Dashboard (UI Enhancement)

### **New UI Components Needed**

**1. Data Quality Card** (on Data Sources page)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sales_Data.csv            Quality: 87% â”‚
â”‚                                     âš ï¸  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 87%        â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                         â”‚
â”‚ Issues Found:                           â”‚
â”‚ â€¢ 3 columns with >10% missing values   â”‚
â”‚ â€¢ Mixed date formats in "order_date"   â”‚
â”‚                                         â”‚
â”‚ [View Details] [Re-process]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**2. Column Quality View** (expandable)
```
Column: revenue_dollars
Quality Score: 92% âœ…

âœ… Completeness: 98% (245/250 rows)
âœ… Consistency: 100% (all currency format)
âœ… Validity: 100% (all positive numbers)
âš ï¸  Outliers: 3 values clipped (>$100k)

Type: currency â†’ float
Imputation: None needed
```

**3. Cleaning Report Modal**
```
Data Processing Report

âœ… Structural Issues Fixed:
   â€¢ Removed 5 empty rows
   â€¢ Removed 2 empty columns
   â€¢ Merged 1 multi-header row

âœ… Columns Normalized:
   â€¢ "Revenue ($)" â†’ "revenue_dollars"
   â€¢ "Customer ID" â†’ "customer_id"
   â€¢ 8 more...

âš ï¸  Data Cleaning Applied:
   â€¢ Imputed 12 missing values (median)
   â€¢ Converted 45 currency strings
   â€¢ Clipped 3 outliers

Quality Score: 87% (Good)

[Download Report] [Close]
```

---

## ğŸ”„ Pipeline Flow Diagram

```
USER UPLOADS FILE
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 1: INGEST  â”‚ â†’ Store RAW copy (immutable)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 2: VALIDATEâ”‚ â†’ Check structure, remove empty
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 3: COLUMNS â”‚ â†’ Normalize names, handle duplicates
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 4: TYPES   â”‚ â†’ Detect & cast types intelligently
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 5: CLEAN   â”‚ â†’ Handle nulls, outliers, duplicates
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 6: QUALITY â”‚ â†’ Calculate metrics, flag issues
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 7: STORE   â”‚ â†’ Parquet + metadata + preview
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
  ANALYSIS-READY
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ AI Queries  â”‚ â†’ OpenAI uses clean data
  â”‚ Accurate!   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ Key Benefits

### **For Users:**
1. **Accurate Analysis**: Clean data = accurate AI insights
2. **Transparency**: See what was cleaned/changed
3. **Confidence**: Quality scores show data reliability
4. **Guidance**: Flagged issues help improve data quality
5. **Speed**: Optimized storage = faster queries

### **For System:**
1. **Reliability**: Defensive programming prevents crashes
2. **Performance**: Parquet storage = 10x faster queries
3. **Scalability**: Handle large files (100MB+) efficiently
4. **Maintainability**: Modular layers, easy to update
5. **Cost Reduction**: Better data = fewer API retries

---

## ğŸš€ Next Steps

1. **Review this architecture** - Confirm approach fits your vision
2. **Examine current implementation** - See what's already done
3. **Prioritize layers** - Which to implement first
4. **Plan UI updates** - Data quality dashboard design
5. **Start coding** - Implement layer by layer

---

**End of Architecture Document**
